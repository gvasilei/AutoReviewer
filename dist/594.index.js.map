{"version":3,"file":"594.index.js","mappings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jFA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://typescript-action/./node_modules/langchain/dist/util/event-source-parse.js","webpack://typescript-action/./node_modules/langchain/dist/util/axios-fetch-adapter.js","webpack://typescript-action/./node_modules/langchain/dist/chat_models/base.js","webpack://typescript-action/./node_modules/langchain/dist/chat_models/openai.js"],"sourcesContent":["/* eslint-disable prefer-template */\n/* eslint-disable default-case */\n/* eslint-disable no-plusplus */\n// Adapted from https://github.com/gfortaine/fetch-event-source/blob/main/src/parse.ts\n// due to a packaging issue in the original.\n// MIT License\nexport const EventStreamContentType = \"text/event-stream\";\n/**\n * Converts a ReadableStream into a callback pattern.\n * @param stream The input ReadableStream.\n * @param onChunk A function that will be called on each new byte chunk in the stream.\n * @returns {Promise<void>} A promise that will be resolved when the stream closes.\n */\nexport async function getBytes(stream, onChunk) {\n    const reader = stream.getReader();\n    let result;\n    // eslint-disable-next-line no-cond-assign\n    while (!(result = await reader.read()).done) {\n        onChunk(result.value);\n    }\n}\n/**\n * Parses arbitary byte chunks into EventSource line buffers.\n * Each line should be of the format \"field: value\" and ends with \\r, \\n, or \\r\\n.\n * @param onLine A function that will be called on each new EventSource line.\n * @returns A function that should be called for each incoming byte chunk.\n */\nexport function getLines(onLine) {\n    let buffer;\n    let position; // current read position\n    let fieldLength; // length of the `field` portion of the line\n    let discardTrailingNewline = false;\n    // return a function that can process each incoming byte chunk:\n    return function onChunk(arr) {\n        if (buffer === undefined) {\n            buffer = arr;\n            position = 0;\n            fieldLength = -1;\n        }\n        else {\n            // we're still parsing the old line. Append the new bytes into buffer:\n            buffer = concat(buffer, arr);\n        }\n        const bufLength = buffer.length;\n        let lineStart = 0; // index where the current line starts\n        while (position < bufLength) {\n            if (discardTrailingNewline) {\n                if (buffer[position] === 10 /* ControlChars.NewLine */) {\n                    lineStart = ++position; // skip to next char\n                }\n                discardTrailingNewline = false;\n            }\n            // start looking forward till the end of line:\n            let lineEnd = -1; // index of the \\r or \\n char\n            for (; position < bufLength && lineEnd === -1; ++position) {\n                switch (buffer[position]) {\n                    case 58 /* ControlChars.Colon */:\n                        if (fieldLength === -1) {\n                            // first colon in line\n                            fieldLength = position - lineStart;\n                        }\n                        break;\n                    // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n                    // @ts-ignore:7029 \\r case below should fallthrough to \\n:\n                    case 13 /* ControlChars.CarriageReturn */:\n                        discardTrailingNewline = true;\n                    // eslint-disable-next-line no-fallthrough\n                    case 10 /* ControlChars.NewLine */:\n                        lineEnd = position;\n                        break;\n                }\n            }\n            if (lineEnd === -1) {\n                // We reached the end of the buffer but the line hasn't ended.\n                // Wait for the next arr and then continue parsing:\n                break;\n            }\n            // we've reached the line end, send it out:\n            onLine(buffer.subarray(lineStart, lineEnd), fieldLength);\n            lineStart = position; // we're now on the next line\n            fieldLength = -1;\n        }\n        if (lineStart === bufLength) {\n            buffer = undefined; // we've finished reading it\n        }\n        else if (lineStart !== 0) {\n            // Create a new view into buffer beginning at lineStart so we don't\n            // need to copy over the previous lines when we get the new arr:\n            buffer = buffer.subarray(lineStart);\n            position -= lineStart;\n        }\n    };\n}\n/**\n * Parses line buffers into EventSourceMessages.\n * @param onId A function that will be called on each `id` field.\n * @param onRetry A function that will be called on each `retry` field.\n * @param onMessage A function that will be called on each message.\n * @returns A function that should be called for each incoming line buffer.\n */\nexport function getMessages(onMessage, onId, onRetry) {\n    let message = newMessage();\n    const decoder = new TextDecoder();\n    // return a function that can process each incoming line buffer:\n    return function onLine(line, fieldLength) {\n        if (line.length === 0) {\n            // empty line denotes end of message. Trigger the callback and start a new message:\n            onMessage?.(message);\n            message = newMessage();\n        }\n        else if (fieldLength > 0) {\n            // exclude comments and lines with no values\n            // line is of format \"<field>:<value>\" or \"<field>: <value>\"\n            // https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation\n            const field = decoder.decode(line.subarray(0, fieldLength));\n            const valueOffset = fieldLength + (line[fieldLength + 1] === 32 /* ControlChars.Space */ ? 2 : 1);\n            const value = decoder.decode(line.subarray(valueOffset));\n            switch (field) {\n                case \"data\":\n                    // if this message already has data, append the new value to the old.\n                    // otherwise, just set to the new value:\n                    message.data = message.data ? message.data + \"\\n\" + value : value; // otherwise,\n                    break;\n                case \"event\":\n                    message.event = value;\n                    break;\n                case \"id\":\n                    onId?.((message.id = value));\n                    break;\n                case \"retry\": {\n                    const retry = parseInt(value, 10);\n                    if (!Number.isNaN(retry)) {\n                        // per spec, ignore non-integers\n                        onRetry?.((message.retry = retry));\n                    }\n                    break;\n                }\n            }\n        }\n    };\n}\nfunction concat(a, b) {\n    const res = new Uint8Array(a.length + b.length);\n    res.set(a);\n    res.set(b, a.length);\n    return res;\n}\nfunction newMessage() {\n    // data, event, and id must be initialized to empty strings:\n    // https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation\n    // retry should be initialized to undefined so we return a consistent shape\n    // to the js engine all the time: https://mathiasbynens.be/notes/shapes-ics#takeaways\n    return {\n        data: \"\",\n        event: \"\",\n        id: \"\",\n        retry: undefined,\n    };\n}\n","/* eslint-disable no-plusplus */\n/* eslint-disable prefer-template */\n/* eslint-disable prefer-arrow-callback */\n/* eslint-disable no-var */\n/* eslint-disable vars-on-top */\n/* eslint-disable no-param-reassign */\n/* eslint-disable import/no-extraneous-dependencies */\n/**\n * This is copied from @vespaiach/axios-fetch-adapter, which exposes an ESM\n * module without setting the \"type\" field in package.json.\n */\nimport axios from \"axios\";\nimport { EventStreamContentType, getLines, getBytes, getMessages, } from \"./event-source-parse.js\";\nfunction tryJsonStringify(data) {\n    try {\n        return JSON.stringify(data);\n    }\n    catch (e) {\n        return data;\n    }\n}\n/**\n * In order to avoid import issues with axios 1.x, copying here the internal\n * utility functions that we used to import directly from axios.\n */\n// Copied from axios/lib/core/settle.js\nfunction settle(resolve, reject, response) {\n    const { validateStatus } = response.config;\n    if (!response.status || !validateStatus || validateStatus(response.status)) {\n        resolve(response);\n    }\n    else {\n        reject(createError(`Request failed with status code ${response.status} and body ${typeof response.data === \"string\"\n            ? response.data\n            : tryJsonStringify(response.data)}`, response.config, null, response.request, response));\n    }\n}\n// Copied from axios/lib/helpers/isAbsoluteURL.js\nfunction isAbsoluteURL(url) {\n    // A URL is considered absolute if it begins with \"<scheme>://\" or \"//\" (protocol-relative URL).\n    // RFC 3986 defines scheme name as a sequence of characters beginning with a letter and followed\n    // by any combination of letters, digits, plus, period, or hyphen.\n    return /^([a-z][a-z\\d+\\-.]*:)?\\/\\//i.test(url);\n}\n// Copied from axios/lib/helpers/combineURLs.js\nfunction combineURLs(baseURL, relativeURL) {\n    return relativeURL\n        ? baseURL.replace(/\\/+$/, \"\") + \"/\" + relativeURL.replace(/^\\/+/, \"\")\n        : baseURL;\n}\n// Copied from axios/lib/helpers/buildURL.js\nfunction encode(val) {\n    return encodeURIComponent(val)\n        .replace(/%3A/gi, \":\")\n        .replace(/%24/g, \"$\")\n        .replace(/%2C/gi, \",\")\n        .replace(/%20/g, \"+\")\n        .replace(/%5B/gi, \"[\")\n        .replace(/%5D/gi, \"]\");\n}\nfunction buildURL(url, params, paramsSerializer) {\n    if (!params) {\n        return url;\n    }\n    var serializedParams;\n    if (paramsSerializer) {\n        serializedParams = paramsSerializer(params);\n    }\n    else if (isURLSearchParams(params)) {\n        serializedParams = params.toString();\n    }\n    else {\n        var parts = [];\n        forEach(params, function serialize(val, key) {\n            if (val === null || typeof val === \"undefined\") {\n                return;\n            }\n            if (isArray(val)) {\n                key = `${key}[]`;\n            }\n            else {\n                val = [val];\n            }\n            forEach(val, function parseValue(v) {\n                if (isDate(v)) {\n                    v = v.toISOString();\n                }\n                else if (isObject(v)) {\n                    v = JSON.stringify(v);\n                }\n                parts.push(`${encode(key)}=${encode(v)}`);\n            });\n        });\n        serializedParams = parts.join(\"&\");\n    }\n    if (serializedParams) {\n        var hashmarkIndex = url.indexOf(\"#\");\n        if (hashmarkIndex !== -1) {\n            url = url.slice(0, hashmarkIndex);\n        }\n        url += (url.indexOf(\"?\") === -1 ? \"?\" : \"&\") + serializedParams;\n    }\n    return url;\n}\n// Copied from axios/lib/core/buildFullPath.js\nfunction buildFullPath(baseURL, requestedURL) {\n    if (baseURL && !isAbsoluteURL(requestedURL)) {\n        return combineURLs(baseURL, requestedURL);\n    }\n    return requestedURL;\n}\n// Copied from axios/lib/utils.js\nfunction isUndefined(val) {\n    return typeof val === \"undefined\";\n}\nfunction isObject(val) {\n    return val !== null && typeof val === \"object\";\n}\nfunction isDate(val) {\n    return toString.call(val) === \"[object Date]\";\n}\nfunction isURLSearchParams(val) {\n    return toString.call(val) === \"[object URLSearchParams]\";\n}\nfunction isArray(val) {\n    return Array.isArray(val);\n}\nfunction forEach(obj, fn) {\n    // Don't bother if no value provided\n    if (obj === null || typeof obj === \"undefined\") {\n        return;\n    }\n    // Force an array if not already something iterable\n    if (typeof obj !== \"object\") {\n        obj = [obj];\n    }\n    if (isArray(obj)) {\n        // Iterate over array values\n        for (var i = 0, l = obj.length; i < l; i++) {\n            fn.call(null, obj[i], i, obj);\n        }\n    }\n    else {\n        // Iterate over object keys\n        for (var key in obj) {\n            if (Object.prototype.hasOwnProperty.call(obj, key)) {\n                fn.call(null, obj[key], key, obj);\n            }\n        }\n    }\n}\nfunction isFormData(val) {\n    return toString.call(val) === \"[object FormData]\";\n}\n// TODO this needs to be fixed to run in newer browser-like environments\n// https://github.com/vespaiach/axios-fetch-adapter/issues/20#issue-1396365322\nfunction isStandardBrowserEnv() {\n    if (typeof navigator !== \"undefined\" &&\n        // eslint-disable-next-line no-undef\n        (navigator.product === \"ReactNative\" ||\n            // eslint-disable-next-line no-undef\n            navigator.product === \"NativeScript\" ||\n            // eslint-disable-next-line no-undef\n            navigator.product === \"NS\")) {\n        return false;\n    }\n    return typeof window !== \"undefined\" && typeof document !== \"undefined\";\n}\n/**\n * - Create a request object\n * - Get response body\n * - Check if timeout\n */\nexport default async function fetchAdapter(config) {\n    const request = createRequest(config);\n    const data = await getResponse(request, config);\n    return new Promise((resolve, reject) => {\n        if (data instanceof Error) {\n            reject(data);\n        }\n        else {\n            // eslint-disable-next-line no-unused-expressions\n            Object.prototype.toString.call(config.settle) === \"[object Function]\"\n                ? config.settle(resolve, reject, data)\n                : settle(resolve, reject, data);\n        }\n    });\n}\n/**\n * Fetch API stage two is to get response body. This funtion tries to retrieve\n * response body based on response's type\n */\nasync function getResponse(request, config) {\n    let stageOne;\n    try {\n        stageOne = await fetch(request);\n    }\n    catch (e) {\n        if (e && e.name === \"AbortError\") {\n            return createError(\"Request aborted\", config, \"ECONNABORTED\", request);\n        }\n        if (e && e.name === \"TimeoutError\") {\n            return createError(\"Request timeout\", config, \"ECONNABORTED\", request);\n        }\n        return createError(\"Network Error\", config, \"ERR_NETWORK\", request);\n    }\n    const headers = {};\n    stageOne.headers.forEach((value, key) => {\n        headers[key] = value;\n    });\n    const response = {\n        ok: stageOne.ok,\n        status: stageOne.status,\n        statusText: stageOne.statusText,\n        headers,\n        config,\n        request,\n    };\n    if (stageOne.status >= 200 && stageOne.status !== 204) {\n        if (config.responseType === \"stream\") {\n            const contentType = stageOne.headers.get(\"content-type\");\n            if (!contentType?.startsWith(EventStreamContentType)) {\n                // If the content-type is not stream, response is most likely an error\n                if (stageOne.status >= 400) {\n                    // If the error is a JSON, parse it. Otherwise, return as text\n                    if (contentType?.startsWith(\"application/json\")) {\n                        response.data = await stageOne.json();\n                        return response;\n                    }\n                    else {\n                        response.data = await stageOne.text();\n                        return response;\n                    }\n                }\n                // If the non-stream response is also not an error, throw\n                throw new Error(`Expected content-type to be ${EventStreamContentType}, Actual: ${contentType}`);\n            }\n            await getBytes(stageOne.body, getLines(getMessages(config.onmessage)));\n        }\n        else {\n            switch (config.responseType) {\n                case \"arraybuffer\":\n                    response.data = await stageOne.arrayBuffer();\n                    break;\n                case \"blob\":\n                    response.data = await stageOne.blob();\n                    break;\n                case \"json\":\n                    response.data = await stageOne.json();\n                    break;\n                case \"formData\":\n                    response.data = await stageOne.formData();\n                    break;\n                default:\n                    response.data = await stageOne.text();\n                    break;\n            }\n        }\n    }\n    return response;\n}\n/**\n * This function will create a Request object based on configuration's axios\n */\nfunction createRequest(config) {\n    const headers = new Headers(config.headers);\n    // HTTP basic authentication\n    if (config.auth) {\n        const username = config.auth.username || \"\";\n        const password = config.auth.password\n            ? decodeURI(encodeURIComponent(config.auth.password))\n            : \"\";\n        headers.set(\"Authorization\", `Basic ${btoa(`${username}:${password}`)}`);\n    }\n    const method = config.method.toUpperCase();\n    const options = {\n        headers,\n        method,\n    };\n    if (method !== \"GET\" && method !== \"HEAD\") {\n        options.body = config.data;\n        // In these cases the browser will automatically set the correct Content-Type,\n        // but only if that header hasn't been set yet. So that's why we're deleting it.\n        if (isFormData(options.body) && isStandardBrowserEnv()) {\n            headers.delete(\"Content-Type\");\n        }\n    }\n    // Some `fetch` implementations will override the Content-Type to text/plain\n    // when body is a string.\n    // See https://github.com/hwchase17/langchainjs/issues/1010\n    if (typeof options.body === \"string\") {\n        options.body = new TextEncoder().encode(options.body);\n    }\n    if (config.mode) {\n        options.mode = config.mode;\n    }\n    if (config.cache) {\n        options.cache = config.cache;\n    }\n    if (config.integrity) {\n        options.integrity = config.integrity;\n    }\n    if (config.redirect) {\n        options.redirect = config.redirect;\n    }\n    if (config.referrer) {\n        options.referrer = config.referrer;\n    }\n    if (config.timeout && config.timeout > 0) {\n        options.signal = AbortSignal.timeout(config.timeout);\n    }\n    if (config.signal) {\n        // this overrides the timeout signal if both are set\n        options.signal = config.signal;\n    }\n    // This config is similar to XHRâ€™s withCredentials flag, but with three available values instead of two.\n    // So if withCredentials is not set, default value 'same-origin' will be used\n    if (!isUndefined(config.withCredentials)) {\n        options.credentials = config.withCredentials ? \"include\" : \"omit\";\n    }\n    // for streaming\n    if (config.responseType === \"stream\") {\n        options.headers.set(\"Accept\", EventStreamContentType);\n    }\n    const fullPath = buildFullPath(config.baseURL, config.url);\n    const url = buildURL(fullPath, config.params, config.paramsSerializer);\n    // Expected browser to throw error if there is any wrong configuration value\n    return new Request(url, options);\n}\n/**\n * Note:\n *\n *   From version >= 0.27.0, createError function is replaced by AxiosError class.\n *   So I copy the old createError function here for backward compatible.\n *\n *\n *\n * Create an Error with the specified message, config, error code, request and response.\n *\n * @param {string} message The error message.\n * @param {Object} config The config.\n * @param {string} [code] The error code (for example, 'ECONNABORTED').\n * @param {Object} [request] The request.\n * @param {Object} [response] The response.\n * @returns {Error} The created error.\n */\nfunction createError(message, config, code, request, response) {\n    if (axios.AxiosError && typeof axios.AxiosError === \"function\") {\n        return new axios.AxiosError(message, axios.AxiosError[code], config, request, response);\n    }\n    const error = new Error(message);\n    return enhanceError(error, config, code, request, response);\n}\n/**\n *\n * Note:\n *\n *   This function is for backward compatible.\n *\n *\n * Update an Error with the specified config, error code, and response.\n *\n * @param {Error} error The error to update.\n * @param {Object} config The config.\n * @param {string} [code] The error code (for example, 'ECONNABORTED').\n * @param {Object} [request] The request.\n * @param {Object} [response] The response.\n * @returns {Error} The error.\n */\nfunction enhanceError(error, config, code, request, response) {\n    error.config = config;\n    if (code) {\n        error.code = code;\n    }\n    error.request = request;\n    error.response = response;\n    error.isAxiosError = true;\n    error.toJSON = function toJSON() {\n        return {\n            // Standard\n            message: this.message,\n            name: this.name,\n            // Microsoft\n            description: this.description,\n            number: this.number,\n            // Mozilla\n            fileName: this.fileName,\n            lineNumber: this.lineNumber,\n            columnNumber: this.columnNumber,\n            stack: this.stack,\n            // Axios\n            config: this.config,\n            code: this.code,\n            status: this.response && this.response.status ? this.response.status : null,\n        };\n    };\n    return error;\n}\n","import { AIChatMessage, RUN_KEY, } from \"../schema/index.js\";\nimport { BaseLanguageModel, } from \"../base_language/index.js\";\nimport { CallbackManager, } from \"../callbacks/manager.js\";\nexport class BaseChatModel extends BaseLanguageModel {\n    constructor(fields) {\n        super(fields);\n    }\n    async generate(messages, options, callbacks) {\n        const generations = [];\n        const llmOutputs = [];\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else if (options?.timeout && !options.signal) {\n            parsedOptions = {\n                ...options,\n                signal: AbortSignal.timeout(options.timeout),\n            };\n        }\n        else {\n            parsedOptions = options ?? {};\n        }\n        const callbackManager_ = await CallbackManager.configure(callbacks, this.callbacks, { verbose: this.verbose });\n        const runManager = await callbackManager_?.handleChatModelStart({ name: this._llmType() }, messages);\n        try {\n            const results = await Promise.all(messages.map((messageList) => this._generate(messageList, parsedOptions, runManager)));\n            for (const result of results) {\n                if (result.llmOutput) {\n                    llmOutputs.push(result.llmOutput);\n                }\n                generations.push(result.generations);\n            }\n        }\n        catch (err) {\n            await runManager?.handleLLMError(err);\n            throw err;\n        }\n        const output = {\n            generations,\n            llmOutput: llmOutputs.length\n                ? this._combineLLMOutput?.(...llmOutputs)\n                : undefined,\n        };\n        await runManager?.handleLLMEnd(output);\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManager ? { runId: runManager?.runId } : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    _modelType() {\n        return \"base_chat_model\";\n    }\n    async generatePrompt(promptValues, options, callbacks) {\n        const promptMessages = promptValues.map((promptValue) => promptValue.toChatMessages());\n        return this.generate(promptMessages, options, callbacks);\n    }\n    async call(messages, options, callbacks) {\n        const result = await this.generate([messages], options, callbacks);\n        const generations = result.generations;\n        return generations[0][0].message;\n    }\n    async callPrompt(promptValue, options, callbacks) {\n        const promptMessages = promptValue.toChatMessages();\n        return this.call(promptMessages, options, callbacks);\n    }\n}\nexport class SimpleChatModel extends BaseChatModel {\n    async _generate(messages, options, runManager) {\n        const text = await this._call(messages, options, runManager);\n        const message = new AIChatMessage(text);\n        return {\n            generations: [\n                {\n                    text: message.text,\n                    message,\n                },\n            ],\n        };\n    }\n}\n","import { isNode } from \"browser-or-node\";\nimport { Configuration, OpenAIApi, } from \"openai\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { BaseChatModel } from \"./base.js\";\nimport { AIChatMessage, ChatMessage, HumanChatMessage, SystemChatMessage, } from \"../schema/index.js\";\nimport { getModelNameForTiktoken } from \"../base_language/count_tokens.js\";\nfunction messageTypeToOpenAIRole(type) {\n    switch (type) {\n        case \"system\":\n            return \"system\";\n        case \"ai\":\n            return \"assistant\";\n        case \"human\":\n            return \"user\";\n        default:\n            throw new Error(`Unknown message type: ${type}`);\n    }\n}\nfunction openAIResponseToChatMessage(role, text) {\n    switch (role) {\n        case \"user\":\n            return new HumanChatMessage(text);\n        case \"assistant\":\n            return new AIChatMessage(text);\n        case \"system\":\n            return new SystemChatMessage(text);\n        default:\n            return new ChatMessage(text, role ?? \"unknown\");\n    }\n}\n/**\n * Wrapper around OpenAI large language models that use the Chat endpoint.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure you should have the `openai` package installed, with the\n * `AZURE_OPENAI_API_KEY`,\n * `AZURE_OPENAI_API_INSTANCE_NAME`,\n * `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n * and `AZURE_OPENAI_API_VERSION` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/chat/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n */\nexport class ChatOpenAI extends BaseChatModel {\n    get callKeys() {\n        return [\"stop\", \"signal\", \"timeout\", \"options\"];\n    }\n    constructor(fields, configuration) {\n        super(fields ?? {});\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"frequencyPenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"presencePenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"n\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"logitBias\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"gpt-3.5-turbo\"\n        });\n        Object.defineProperty(this, \"modelKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stop\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"timeout\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiVersion\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiInstanceName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiDeploymentName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"clientConfig\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        const apiKey = fields?.openAIApiKey ??\n            (typeof process !== \"undefined\"\n                ? // eslint-disable-next-line no-process-env\n                    process.env?.OPENAI_API_KEY\n                : undefined);\n        const azureApiKey = fields?.azureOpenAIApiKey ??\n            (typeof process !== \"undefined\"\n                ? // eslint-disable-next-line no-process-env\n                    process.env?.AZURE_OPENAI_API_KEY\n                : undefined);\n        if (!azureApiKey && !apiKey) {\n            throw new Error(\"(Azure) OpenAI API key not found\");\n        }\n        const azureApiInstanceName = fields?.azureOpenAIApiInstanceName ??\n            (typeof process !== \"undefined\"\n                ? // eslint-disable-next-line no-process-env\n                    process.env?.AZURE_OPENAI_API_INSTANCE_NAME\n                : undefined);\n        const azureApiDeploymentName = fields?.azureOpenAIApiDeploymentName ??\n            (typeof process !== \"undefined\"\n                ? // eslint-disable-next-line no-process-env\n                    process.env?.AZURE_OPENAI_API_DEPLOYMENT_NAME\n                : undefined);\n        const azureApiVersion = fields?.azureOpenAIApiVersion ??\n            (typeof process !== \"undefined\"\n                ? // eslint-disable-next-line no-process-env\n                    process.env?.AZURE_OPENAI_API_VERSION\n                : undefined);\n        this.modelName = fields?.modelName ?? this.modelName;\n        this.modelKwargs = fields?.modelKwargs ?? {};\n        this.timeout = fields?.timeout;\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.topP = fields?.topP ?? this.topP;\n        this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n        this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n        this.maxTokens = fields?.maxTokens;\n        this.n = fields?.n ?? this.n;\n        this.logitBias = fields?.logitBias;\n        this.stop = fields?.stop;\n        this.streaming = fields?.streaming ?? false;\n        this.azureOpenAIApiVersion = azureApiVersion;\n        this.azureOpenAIApiKey = azureApiKey;\n        this.azureOpenAIApiInstanceName = azureApiInstanceName;\n        this.azureOpenAIApiDeploymentName = azureApiDeploymentName;\n        if (this.streaming && this.n > 1) {\n            throw new Error(\"Cannot stream results when n > 1\");\n        }\n        if (this.azureOpenAIApiKey) {\n            if (!this.azureOpenAIApiInstanceName) {\n                throw new Error(\"Azure OpenAI API instance name not found\");\n            }\n            if (!this.azureOpenAIApiDeploymentName) {\n                throw new Error(\"Azure OpenAI API deployment name not found\");\n            }\n            if (!this.azureOpenAIApiVersion) {\n                throw new Error(\"Azure OpenAI API version not found\");\n            }\n        }\n        this.clientConfig = {\n            apiKey,\n            ...configuration,\n        };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams() {\n        return {\n            model: this.modelName,\n            temperature: this.temperature,\n            top_p: this.topP,\n            frequency_penalty: this.frequencyPenalty,\n            presence_penalty: this.presencePenalty,\n            max_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n            n: this.n,\n            logit_bias: this.logitBias,\n            stop: this.stop,\n            stream: this.streaming,\n            ...this.modelKwargs,\n        };\n    }\n    /** @ignore */\n    _identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams() {\n        return this._identifyingParams();\n    }\n    /** @ignore */\n    async _generate(messages, options, runManager) {\n        const tokenUsage = {};\n        if (this.stop && options?.stop) {\n            throw new Error(\"Stop found in input and default params\");\n        }\n        const params = this.invocationParams();\n        params.stop = options?.stop ?? params.stop;\n        const messagesMapped = messages.map((message) => ({\n            role: messageTypeToOpenAIRole(message._getType()),\n            content: message.text,\n            name: message.name,\n        }));\n        const data = params.stream\n            ? await new Promise((resolve, reject) => {\n                let response;\n                let rejected = false;\n                let resolved = false;\n                this.completionWithRetry({\n                    ...params,\n                    messages: messagesMapped,\n                }, {\n                    signal: options?.signal,\n                    ...options?.options,\n                    adapter: fetchAdapter,\n                    responseType: \"stream\",\n                    onmessage: (event) => {\n                        if (event.data?.trim?.() === \"[DONE]\") {\n                            if (resolved) {\n                                return;\n                            }\n                            resolved = true;\n                            resolve(response);\n                        }\n                        else {\n                            const message = JSON.parse(event.data);\n                            // on the first message set the response properties\n                            if (!response) {\n                                response = {\n                                    id: message.id,\n                                    object: message.object,\n                                    created: message.created,\n                                    model: message.model,\n                                    choices: [],\n                                };\n                            }\n                            // on all messages, update choice\n                            for (const part of message.choices) {\n                                if (part != null) {\n                                    let choice = response.choices.find((c) => c.index === part.index);\n                                    if (!choice) {\n                                        choice = {\n                                            index: part.index,\n                                            finish_reason: part.finish_reason ?? undefined,\n                                        };\n                                        response.choices[part.index] = choice;\n                                    }\n                                    if (!choice.message) {\n                                        choice.message = {\n                                            role: part.delta\n                                                ?.role,\n                                            content: part.delta?.content ?? \"\",\n                                        };\n                                    }\n                                    choice.message.content += part.delta?.content ?? \"\";\n                                    // TODO this should pass part.index to the callback\n                                    // when that's supported there\n                                    // eslint-disable-next-line no-void\n                                    void runManager?.handleLLMNewToken(part.delta?.content ?? \"\");\n                                }\n                            }\n                            // when all messages are finished, resolve\n                            if (!resolved &&\n                                message.choices.every((c) => c.finish_reason != null)) {\n                                resolved = true;\n                                resolve(response);\n                            }\n                        }\n                    },\n                }).catch((error) => {\n                    if (!rejected) {\n                        rejected = true;\n                        reject(error);\n                    }\n                });\n            })\n            : await this.completionWithRetry({\n                ...params,\n                messages: messagesMapped,\n            }, {\n                signal: options?.signal,\n                ...options?.options,\n            });\n        const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = data.usage ?? {};\n        if (completionTokens) {\n            tokenUsage.completionTokens =\n                (tokenUsage.completionTokens ?? 0) + completionTokens;\n        }\n        if (promptTokens) {\n            tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n        }\n        if (totalTokens) {\n            tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n        }\n        const generations = [];\n        for (const part of data.choices) {\n            const role = part.message?.role ?? undefined;\n            const text = part.message?.content ?? \"\";\n            generations.push({\n                text,\n                message: openAIResponseToChatMessage(role, text),\n            });\n        }\n        return {\n            generations,\n            llmOutput: { tokenUsage },\n        };\n    }\n    async getNumTokensFromMessages(messages) {\n        let totalCount = 0;\n        let tokensPerMessage = 0;\n        let tokensPerName = 0;\n        // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n        if (getModelNameForTiktoken(this.modelName) === \"gpt-3.5-turbo\") {\n            tokensPerMessage = 4;\n            tokensPerName = -1;\n        }\n        else if (getModelNameForTiktoken(this.modelName).startsWith(\"gpt-4\")) {\n            tokensPerMessage = 3;\n            tokensPerName = 1;\n        }\n        const countPerMessage = await Promise.all(messages.map(async (message) => {\n            const textCount = await this.getNumTokens(message.text);\n            const roleCount = await this.getNumTokens(messageTypeToOpenAIRole(message._getType()));\n            const nameCount = message.name !== undefined\n                ? tokensPerName + (await this.getNumTokens(message.name))\n                : 0;\n            const count = textCount + tokensPerMessage + roleCount + nameCount;\n            totalCount += count;\n            return count;\n        }));\n        totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n        return { totalCount, countPerMessage };\n    }\n    /** @ignore */\n    async completionWithRetry(request, options) {\n        if (!this.client) {\n            const endpoint = this.azureOpenAIApiKey\n                ? `https://${this.azureOpenAIApiInstanceName}.openai.azure.com/openai/deployments/${this.azureOpenAIApiDeploymentName}`\n                : this.clientConfig.basePath;\n            const clientConfig = new Configuration({\n                ...this.clientConfig,\n                basePath: endpoint,\n                baseOptions: {\n                    timeout: this.timeout,\n                    ...this.clientConfig.baseOptions,\n                },\n            });\n            this.client = new OpenAIApi(clientConfig);\n        }\n        const axiosOptions = {\n            adapter: isNode ? undefined : fetchAdapter,\n            ...this.clientConfig.baseOptions,\n            ...options,\n        };\n        if (this.azureOpenAIApiKey) {\n            axiosOptions.headers = {\n                \"api-key\": this.azureOpenAIApiKey,\n                ...axiosOptions.headers,\n            };\n            axiosOptions.params = {\n                \"api-version\": this.azureOpenAIApiVersion,\n                ...axiosOptions.params,\n            };\n        }\n        return this.caller\n            .call(this.client.createChatCompletion.bind(this.client), request, axiosOptions)\n            .then((res) => res.data);\n    }\n    _llmType() {\n        return \"openai\";\n    }\n    /** @ignore */\n    _combineLLMOutput(...llmOutputs) {\n        return llmOutputs.reduce((acc, llmOutput) => {\n            if (llmOutput && llmOutput.tokenUsage) {\n                acc.tokenUsage.completionTokens +=\n                    llmOutput.tokenUsage.completionTokens ?? 0;\n                acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n                acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n            }\n            return acc;\n        }, {\n            tokenUsage: {\n                completionTokens: 0,\n                promptTokens: 0,\n                totalTokens: 0,\n            },\n        });\n    }\n}\n"],"names":[],"sourceRoot":""}